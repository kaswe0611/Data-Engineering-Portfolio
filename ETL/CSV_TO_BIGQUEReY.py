import os
import csv
from google.cloud import bigquery
import requests
import pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from datetime import datetime
from io import StringIO

# è¨­å®šä½ çš„GCPæ†‘è­‰èˆ‡å°ˆæ¡ˆè³‡è¨Š
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r"C:\Users\user\Desktop\æ¸¬è©¦bigquery\test-bigquery-468412-88f698064d4f.json" 
project_id = 'test-bigquery-468412' # æ›¿æ›ç‚ºä½ çš„å°ˆæ¡ˆID
dataset_id = 'test_BQ'  
table_id = 'test_BQ_table'

# è¨­å®šè¦ä¿ç•™çš„è³‡æ–™ç­†æ•¸
MAX_RECORDS = 40
# å‡è¨­ä½ çš„æ™‚é–“æˆ³è¨˜æ¬„ä½åç¨±æ˜¯ 'timestamp'
TIMESTAMP_FIELD = 'TimeStamp' 

# å»ºç«‹ BigQuery å®¢æˆ¶ç«¯ç‰©ä»¶
client = bigquery.Client(project=project_id)
table_ref = client.dataset(dataset_id).table(table_id)

BASE_URL = "https://tisvcloud.freeway.gov.tw/history/TDCS/M05A/"
COLUMNS = ["TimeStamp", "GantryFrom", "GantryTo", "VehicleType", "Speed", "Volume"]
TARGET_IDS = {"05F0287N", "05F0055N", "05F0001N"}

def manage_data_retention(client, table_ref, max_records):
    """
    æª¢æŸ¥è³‡æ–™è¡¨ç­†æ•¸ï¼Œå¦‚æœè¶…éé™åˆ¶ï¼Œåˆªé™¤æœ€èˆŠçš„è³‡æ–™ã€‚
    """
    # 1. æŸ¥è©¢ç›®å‰çš„è³‡æ–™ç­†æ•¸
    query = f"SELECT COUNT(*) AS total_rows FROM `{table_ref.project}.{table_ref.dataset_id}.{table_ref.table_id}`"
    query_job = client.query(query)
    result = query_job.result()
    total_rows = next(result).total_rows
    
    print(f"ç›®å‰è³‡æ–™è¡¨ä¸­å…±æœ‰ {total_rows} ç­†è³‡æ–™ã€‚")
    
    # 2. å¦‚æœç­†æ•¸è¶…éæˆ–ç­‰æ–¼ä¸Šé™ï¼Œå‰‡åˆªé™¤å¤šé¤˜çš„è³‡æ–™
    if total_rows >= max_records:
        rows_to_delete = total_rows - max_records + 1 # åˆªé™¤å¤šé¤˜çš„ï¼Œä¸¦ç‚ºæ–°è³‡æ–™é¨°å‡ºä¸€å€‹ä½ç½®
        print(f"è³‡æ–™ç­†æ•¸è¶…éä¸Šé™ï¼Œå°‡åˆªé™¤æœ€èˆŠçš„ {rows_to_delete} ç­†è³‡æ–™ã€‚")
        
        # åˆªé™¤æœ€èˆŠçš„è³‡æ–™
        delete_query = f"""
        DELETE FROM `{table_ref.project}.{table_ref.dataset_id}.{table_ref.table_id}`
        WHERE {TIMESTAMP_FIELD} IN (
            SELECT {TIMESTAMP_FIELD}
            FROM `{table_ref.project}.{table_ref.dataset_id}.{table_ref.table_id}`
            ORDER BY {TIMESTAMP_FIELD} ASC
            LIMIT {rows_to_delete}
        )
        """
        delete_job = client.query(delete_query)
        delete_job.result()
        print("æœ€èˆŠçš„è³‡æ–™å·²åˆªé™¤ã€‚")


# === å·¥å…·: æŠ“å– HTML é€£çµ ===
def get_links_by_suffix(url, suffix):
    res = requests.get(url, verify=False)
    if res.status_code != 200:
        print(f"âŒ é€£ç·šå¤±æ•—: {url} ({res.status_code})")
        return []
    soup = BeautifulSoup(res.text, "html.parser")
    return [
        urljoin(url, a["href"])
        for td in soup.find_all("td", class_="indexcolname")
        for a in td.find_all("a")
        if a["href"].endswith(suffix)
    ]

# === æŠ“ä»Šå¤©æœ€æ–°CSVé€£çµ ===
def get_latest_csv_link():
    today_str = datetime.now().strftime("%Y%m%d") # â† æŠ“ä»Šå¤©æ—¥æœŸ
    date_url = urljoin(BASE_URL, today_str + "/") # â† æ‹¼å‡ºä»Šå¤©çš„è³‡æ–™å¤¾ç¶²å€

    hour_folders = get_links_by_suffix(date_url, "/") # â† æŠ“ä»Šå¤©åº•ä¸‹æ‰€æœ‰å°æ™‚è³‡æ–™å¤¾
    if not hour_folders:
        print("âš ï¸ ç„¡æ³•å–å¾—å°æ™‚è³‡æ–™å¤¾")
        return None, None

    latest_hour_url = sorted(hour_folders)[-1] # â† æŒ‘æœ€æ™šçš„ä¸€å°æ™‚è³‡æ–™å¤¾
    csv_links = get_links_by_suffix(latest_hour_url, ".csv") # â† æŠ“é€™å€‹å°æ™‚çš„æ‰€æœ‰ CSV
    if not csv_links:
        print("âš ï¸ æ²’æœ‰æ‰¾åˆ°CSVæª”")
        return None, None

    return sorted(csv_links)[-1]


# === ä¸‹è¼‰èˆ‡éæ¿¾è³‡æ–™ ===
def process_data_from_url():
    """
    å¾é ç«¯ç¶²å€æŠ“å–ã€éæ¿¾ä¸¦åˆ†çµ„çµ±è¨ˆè³‡æ–™ã€‚
    """
    csv_url = get_latest_csv_link()
    if not csv_url:
        print("âŒ æ²’æœ‰æœ€æ–° CSV æª”æ¡ˆ")
        return None
    
    try:
        print(f"â¬‡ï¸ ä¸‹è¼‰æœ€æ–°æª”æ¡ˆï¼š{csv_url}")
        r = requests.get(csv_url,verify=False, timeout=20)
        if r.status_code != 200:
            print(f"âŒ ç„¡æ³•ä¸‹è¼‰: {csv_url}")
            return None
        df = pd.read_csv(StringIO(r.text), header=None)
        if df.shape[1] != 6:
            print("âš ï¸ æ¬„ä½æ•¸é‡éŒ¯èª¤")
            return None
        df.columns = COLUMNS
        # â¤ éæ¿¾ï¼šGantryFrom âˆˆ TARGET_IDS ä¸” Speed â‰  0
        df = df[(df["GantryFrom"].isin(TARGET_IDS)) & (df["Speed"] != 0)]
        if df.empty:
            print("âš ï¸ ç„¡å¯ç”¨è³‡æ–™é€²è¡Œçµ±è¨ˆ")
            return None
        grouped = df.groupby(["GantryFrom", "GantryTo"]).agg({
            "Speed": "mean",
            "Volume": "sum"
        }).reset_index()

        # âœ… æ–°å¢éæ¿¾ï¼šåªä¿ç•™ GantryTo âˆˆ TARGET_IDS
        grouped = grouped[grouped["GantryTo"].isin(TARGET_IDS)]

        # â¤ å–å¾—æ™‚é–“ï¼ˆç¬¬ä¸€ç­†ï¼‰
        # timestamp = pd.to_datetime(df["TimeStamp"].iloc[0])
        # grouped.insert(0, "TimeStamp", timestamp)
        grouped['TimeStamp'] = pd.to_datetime(df["TimeStamp"].iloc[0])

        grouped['Date'] = grouped['TimeStamp'].dt.strftime('%Y-%m-%d')
        grouped['Time'] = grouped['TimeStamp'].dt.strftime('%H:%M')

        grouped['TimeStamp'] = grouped['TimeStamp'].dt.strftime('%Y-%m-%d %H:%M:%S')

        print("ğŸ“Š åˆ†çµ„çµ±è¨ˆçµæœï¼ˆSpeed å¹³å‡, Volume åŠ ç¸½ï¼‰ï¼š")
        print(grouped)

        return grouped.to_dict('records') # è½‰æ›æˆ BigQuery æ¥å—çš„å­—å…¸åˆ—è¡¨æ ¼å¼
    
    except Exception as e:
        print(f"âš ï¸ è³‡æ–™è™•ç†å¤±æ•—ï¼š{e}")
        return None
    
def upload_to_bigquery(rows_to_insert):
    """
    å°‡è³‡æ–™åŒ¯å…¥ BigQuery è³‡æ–™è¡¨ã€‚
    """
    if not rows_to_insert:
        print("æ²’æœ‰è³‡æ–™å¯ä¾›åŒ¯å…¥ã€‚")
        return
    
    try:

        for row in rows_to_insert:
            # åŸ·è¡Œå‹æ…‹è½‰æ›
            row['Speed'] = int(float(row['Speed']))
            row['Volume'] = int(float(row['Volume']))

        errors = client.insert_rows_json(table_ref, rows_to_insert)
        
        if errors:
            print(f"è³‡æ–™åŒ¯å…¥æ™‚ç™¼ç”ŸéŒ¯èª¤: {errors}")
        else:
            print(f"æˆåŠŸåŒ¯å…¥ {len(rows_to_insert)} ç­†è³‡æ–™åˆ° BigQueryã€‚")
    except Exception as e:
        print(f"åŒ¯å…¥éç¨‹ä¸­ç™¼ç”Ÿç•°å¸¸: {e}")

if __name__ == '__main__':
    # 1. ç®¡ç†è³‡æ–™ä¿ç•™ç­–ç•¥ï¼šå¦‚æœè¶…é40ç­†ï¼Œå‰‡åˆªé™¤æœ€èˆŠçš„
    manage_data_retention(client, table_ref, MAX_RECORDS)
    
    # 2. æŠ“å–æœ€æ–°è³‡æ–™ (é€™è£¡ä½¿ç”¨ CSV æ¨¡æ“¬)
    new_data = process_data_from_url()

    # 3. åŒ¯å…¥ BigQuery
    if new_data:
        upload_to_bigquery(new_data)
